# Image Server Optimization Checklist

## ‚ö†Ô∏è **SAFETY NOTICE**
This plan contains **compatibility risks**. Test each phase on a separate branch before merging to main.

## Phase 1 ‚Äî Precision & Device Selection ‚ö†Ô∏è **HIGH RISK**
**CRITICAL**: `torch.bfloat16` is NOT supported on MPS (Apple Silicon). Current code works due to fallback logic.

- [ ] **Config auto-detection**: Add `select_device_dtype()` helper in `ImageServer.py` to resolve device/dtype automatically.
  - ‚ö†Ô∏è **Risk**: Must handle MPS `bfloat16` ‚Üí `float16` conversion properly
  - üõ°Ô∏è **Mitigation**: Test on M4 Max before deploying, ensure fallback logic preserved
- [ ] **Env overrides**: Support `.env` variables (e.g., `FLUX_DTYPE`, `FLUX_DEVICE`) that feed into `CONFIG` defaults.
- [ ] **Logging**: Emit resolved device/dtype at startup for verification.
- [ ] **Testing**: Verify on MPS, CUDA, and CPU backends before proceeding

## Phase 2 ‚Äî Generator Reuse & Seeding ‚ö†Ô∏è **MEDIUM RISK**
**WARNING**: Generator device mismatches cause `"Expected 'mps:0' generator but found 'cpu'"` crashes.

- [ ] **Generator cache**: Maintain per-device `torch.Generator` instances in `ImageServer.py` instead of recreating per request.
  - ‚ö†Ô∏è **Risk**: Device mismatch between generator and tensors will crash
  - üõ°Ô∏è **Mitigation**: Ensure generator device matches pipeline device, add device validation
- [ ] **Seed control**: Re-seed cached generator only when `request['seed']` provided; return seed in response payload.
- [ ] **Determinism test**: Confirm identical outputs given same seed twice in a row.
- [ ] **Device validation**: Add checks to ensure generator device matches pipeline device

## Phase 3 ‚Äî Pipeline Optimizations ‚ö†Ô∏è **MEDIUM RISK**
**WARNING**: CPU offload changes can break performance or cause crashes with PyTorch compile.

- [ ] **Progress bar**: Disable tqdm via `pipe.set_progress_bar_config(disable=True)` after `_build_pipeline()`.
  - ‚úÖ **Safe**: Low risk optimization
- [ ] **Advanced slicing**: Allow configurable attention/vae slicing chunk sizes via config/env flags.
  - ‚úÖ **Safe**: Existing feature, just making it configurable
- [ ] **Offload modes**: Choose between `enable_model_cpu_offload()` and `enable_sequential_cpu_offload()` based on new config option.
  - ‚ö†Ô∏è **Risk**: Sequential offload is 3x slower, model offload conflicts with PyTorch compile
  - üõ°Ô∏è **Mitigation**: Keep current offload as default, make new modes opt-in only
- [ ] **Performance testing**: Benchmark before/after on M4 Max to ensure no regression
- [ ] **README update**: Document new optimization knobs in `README.md`.

## Phase 4 ‚Äî MPS Error Recovery Enhancements ‚ö†Ô∏è **LOW RISK**
**NOTE**: MPS memory management is still buggy, these help but don't guarantee fixes.

- [ ] **Cache cleanup**: Call `torch.mps.empty_cache()` before retrying after MPS OOM.
  - ‚ö†Ô∏è **Risk**: May not reliably prevent OOM, MPS cache behavior is unpredictable
  - üõ°Ô∏è **Mitigation**: Keep existing fallback logic as primary recovery method
- [ ] **Adaptive fallback**: Optionally switch to `torch.float32` and reduce steps/resolution with detailed `fallback_reason` metadata.
  - ‚úÖ **Safe**: Additive improvement to existing fallback
- [ ] **Cross-backend guard**: Detect similar CUDA OOM messages and reuse fallback logic.
  - ‚úÖ **Safe**: Extends existing logic to other backends

## Phase 5 ‚Äî Concurrency & Locking ‚úÖ **LOW RISK**
- [ ] **Per-model locks**: Replace global `_model_lock` with per-model locks in `ImageServer.py`.
  - ‚ö†Ô∏è **Risk**: Potential deadlocks if not implemented carefully
  - üõ°Ô∏è **Mitigation**: Use timeout-based locks, thorough testing
- [ ] **Semaphore**: Introduce configurable `asyncio.Semaphore` to limit concurrent generations.
  - ‚úÖ **Safe**: Standard concurrency control
- [ ] **Load test**: Run concurrency benchmark on macOS (M-series) and CPU-only setups to validate throughput.

## Phase 6 ‚Äî Configuration Surface ‚úÖ **SAFE**
- [ ] **dotenv alignment**: Extend `.env.example` with new flags (dtype/device/slicing/offload/queue/warmup).
- [ ] **Runtime snapshot**: Update `/v1/models` response to expose active config values.
- [ ] **Config helper**: Centralize env parsing to keep `CONFIG` consistent across modules.

## Phase 7 ‚Äî Warmup Routine ‚úÖ **SAFE**
- [ ] **Config flag**: Introduce `CONFIG['WARMUP_ENABLE']` (and env override) for warmup control.
- [ ] **Warmup run**: After pipeline creation/preload, execute low-resolution, single-step dummy generation when enabled.
- [ ] **Warmup logging**: Log duration and outcome per model.

## Phase 8 ‚Äî Metrics & Logging ‚úÖ **SAFE**
- [ ] **Timing metrics**: Wrap generation path with perf counters, log total/request/save durations.
- [ ] **Structured logging**: Add optional JSON logging when `FLUX_STRUCTURED_LOGS=1`.
- [ ] **Metrics endpoint**: Consider lightweight `/metrics` exposing rolling averages or integrate with Prometheus if feasible.

---

## üöÄ **RECOMMENDED IMPLEMENTATION ORDER**

### **Start Here (Safest)**
1. **Phase 6** - Configuration Surface
2. **Phase 8** - Metrics & Logging  
3. **Phase 7** - Warmup Routine

### **Then (With Caution)**
4. **Phase 4** - MPS Error Recovery (test thoroughly)
5. **Phase 5** - Concurrency & Locking (test with load)

### **Finally (High Risk - Separate Branch)**
6. **Phase 1** - Precision & Device Selection (‚ö†Ô∏è **TEST ON M4 MAX FIRST**)
7. **Phase 3** - Pipeline Optimizations (benchmark performance)
8. **Phase 2** - Generator Reuse (validate device matching)

### **Testing Strategy**
- Create feature branch for each phase
- Test on M4 Max before merging
- Keep rollback plan ready
- Monitor performance metrics after each phase
