# Hugging Face Authentication Token
# Required to download models from Hugging Face Hub
# Get your token from: https://huggingface.co/settings/tokens
# Make sure to accept the model licenses:
# - https://huggingface.co/black-forest-labs/FLUX.1-schnell
# - https://huggingface.co/black-forest-labs/FLUX.1-dev
# - https://huggingface.co/Qwen/Qwen-Image
HUGGINGFACE_HUB_TOKEN=your_token_here

# ============================================================
# MODEL SELECTION
# ============================================================

# Choose which model to load at startup: schnell, dev, or qwen
# Default: schnell
FLUX_MODEL_KEY=schnell

# Optional: Custom Model Override
# Use a custom Hugging Face model ID instead of the predefined models
# FLUX_MODEL_ID=your-custom/model-id

# ============================================================
# DEVICE & PRECISION SETTINGS
# ============================================================

# Data type for model weights: bfloat16, fp16, or fp32
# Note: bfloat16 falls back to fp16 on MPS (Apple Silicon)
# Default: bfloat16
# FLUX_DTYPE=bfloat16

# Device for random number generator: cpu, mps, cuda, or auto
# cpu is recommended to avoid MPS generator bugs in some PyTorch versions
# Default: cpu
# FLUX_GENERATOR_DEVICE=cpu

# ============================================================
# MEMORY OPTIMIZATION
# ============================================================

# Enable attention slicing to reduce memory usage
# Default: 1 (enabled)
# FLUX_ENABLE_SLICING=1

# Enable VAE tiling to reduce memory usage during encoding/decoding
# Default: 1 (enabled)
# FLUX_ENABLE_VAE_TILING=1

# Enable CPU offload to reduce GPU memory usage (slower inference)
# Default: 0 (disabled)
# FLUX_ENABLE_CPU_OFFLOAD=0

# ============================================================
# CACHE & STORAGE
# ============================================================

# Custom cache directory for Hugging Face models
# Default: uses HF default cache (~/.cache/huggingface)
# FLUX_CACHE_DIR=/path/to/cache

# Pin a specific model revision (commit hash or tag)
# Default: latest
# FLUX_REVISION=main

# Model variant (e.g., fp16)
# Default: none
# FLUX_VARIANT=fp16

# ============================================================
# PERFORMANCE & STARTUP
# ============================================================

# Preload all available models at startup (increases startup time)
# Default: 0 (disabled)
# FLUX_PRELOAD_MODELS=0

# Enable warmup generation after loading models
# Runs a low-resolution dummy generation to warm up the pipeline
# Default: 0 (disabled)
# FLUX_WARMUP_ENABLE=0

# ============================================================
# CONCURRENCY & QUEUE
# ============================================================

# Maximum number of concurrent generation requests
# Default: 1 (no limit)
# FLUX_MAX_CONCURRENT_REQUESTS=1

# ============================================================
# LOGGING & DEBUGGING
# ============================================================

# Enable structured JSON logging
# Default: 0 (disabled, uses simple print statements)
# FLUX_STRUCTURED_LOGS=0

# Log level: DEBUG, INFO, WARNING, ERROR
# Default: INFO
# FLUX_LOG_LEVEL=INFO
